# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WkHk84pcxG5yBRn2qyZP_9uI7x0ueTKg
"""

!pip install transformers
!pip install -q torch torchvision torchaudio

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# 모델 이름
model_name = "ProsusAI/finbert"

# 토크나이저 및 모델 불러오기
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 파이프라인 생성 (감성 분석용)
finbert = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# 예제 문장
text = "Tesla shares rose after the company posted strong quarterly earnings."

# 감성 예측
result = finbert(text)
print(result)

# Colab에 kaggle.json 업로드
from google.colab import files
files.upload()  # 여기서 kaggle.json 선택

# .kaggle 폴더로 옮기기
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# 권한 설정
!chmod 600 ~/.kaggle/kaggle.json

# 필요한 데이터셋 다운로드
!kaggle datasets download -d journeyyouyeonkim/microsoft-tesla-finance-news-articles2020-2024

# 압축 해제
!unzip microsoft-tesla-finance-news-articles2020-2024.zip

import pandas as pd

# CSV 파일 읽기
df = pd.read_csv("tsla_articles.csv")

# 앞 5개 행 확인
df.head()

subset = df[['date', 'text']].copy()

# 감성 분석 함수 정의
def analyze_sentiment(text):
    result = finbert(text[:512])[0]
    return pd.Series([result['label'], round(result['score'], 4)])

# sentiment와 confidence 컬럼 생성
subset[['sentiment', 'confidence']] = subset['text'].apply(analyze_sentiment)

# 텍스트는 제거하고 최종 결과만 남김
final_df = subset[['date', 'sentiment', 'confidence']]

final_df = final_df.sort_values(by='date', ascending=True).reset_index(drop=True)

print(final_df.head(5))
print(final_df.tail(5))

from google.colab import files
files.upload()  # 여기서 kaggle.json 업로드
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# 해당 테슬라 주가 데이터셋 다운로드
!kaggle datasets download -d iamtanmayshukla/tesla-stocks-dataset

# 압축 풀기
!unzip tesla-stocks-dataset.zip

import pandas as pd

# CSV 파일 읽기
df = pd.read_csv("TSLA-2.csv")

# 앞 5개 행 확인
df.head()
df.tail()

df['Date'] = pd.to_datetime(df['Date'])

filtered_stock = df[
    (df['Date'] >= '2020-01-01') &
    (df['Date'] <= '2024-07-5')
].copy()

filtered_stock.head()

# 감정 데이터와 주가 데이터 모두 datetime 형식으로 통일
final_df['date'] = pd.to_datetime(final_df['date'])
filtered_stock['Date'] = pd.to_datetime(filtered_stock['Date'])

merged = pd.merge(
    final_df,          # 감정 분석 데이터 (왼쪽 기준)
    filtered_stock,          # 주가 데이터
    how='left',        # 감정 날짜 기준으로 붙이고, 주가 없으면 NaN
    left_on='date',
    right_on='Date'
)

merged.drop(columns=['Date'], inplace=True)

merged.tail(10)

import pandas as pd
import numpy as np
from tqdm import tqdm

# 데이터프레임: df (이미 날짜 포함)
# 컬럼: 'date', 'sentiment', 'confidence'

df = merged
df['date'] = pd.to_datetime(df['date'])
df.set_index('date', inplace=True)

sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
df['sentiment_score'] = df['sentiment'].map(sentiment_map)

weights = np.linspace(1.0, 0.1, 7)  # 1.0 → 0.1까지 7단계로 줄어듦

df['sentiment_influence'] = 0.0

for date, row in tqdm(df.iterrows(), total=len(df)):
    # neutral 또는 NaN은 건너뜀
    if pd.isna(row['sentiment_score']) or pd.isna(row['confidence']):
        continue

    base = row['sentiment_score'] * row['confidence']

    # 7일 동안 가중치 적용해서 누적
    for i in range(7):
        apply_date = date + pd.Timedelta(days=i)
        if apply_date in df.index:
            df.at[apply_date, 'sentiment_influence'] += base * weights[i]

df[['sentiment', 'confidence', 'sentiment_score', 'sentiment_influence']].head(15)

features = ['sentiment_influence', 'Open', 'High', 'Low', 'Adj Close', 'Volume']
target   = 'Close'
data = df[ features + [target] ].dropna()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaled = scaler.fit_transform(data)

import numpy as np

def create_sequences(arr, seq_len):
    X, y = [], []
    for i in range(len(arr) - seq_len):
        X.append(arr[i:i+seq_len, :-1])   # 모든 피처
        y.append(arr[i+seq_len, -1])      # Close만
    return np.array(X), np.array(y)

SEQ_LEN = 10
X, y = create_sequences(scaled, SEQ_LEN)

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    LSTM(64, input_shape=(SEQ_LEN, X.shape[2])),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()

history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val)
)

# X_val: (샘플수, seq_len, 피처수)
# y_val: (샘플수,)
y_pred = model.predict(X_val)  # 예측값 (스케일된 종가)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# MSE, MAE
mse = mean_squared_error(y_val, y_pred)
mae = mean_absolute_error(y_val, y_pred)
rmse = np.sqrt(mse)

print(f"MSE: {mse:.5f}")
print(f"RMSE: {rmse:.5f}")
print(f"MAE: {mae:.5f}")

import matplotlib.pyplot as plt

plt.figure()
plt.plot(y_val, label="real Close")
plt.plot(y_pred, label="predicted Close")
plt.title("Actual vs Predicted Close (Scaled)")
plt.legend()
plt.show()